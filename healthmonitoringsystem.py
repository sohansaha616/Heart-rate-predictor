# -*- coding: utf-8 -*-
"""Healthmonitoringsystem.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KxdXUm40TFtoQbWIFZ2RlFq1UFrZJ3F4
"""

# Install required packages
!pip install xgboost

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from scipy.stats import ttest_rel, pearsonr
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Dropout

# File upload
from google.colab import files
uploaded = files.upload()  # Upload S3_E4.zip

# Extract ZIP
import zipfile, os
zip_path = "S3_E4.zip"
extract_path = "S3_E4"
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extracted files:", os.listdir(extract_path))

# Function to load Empatica signal with timestamp
def load_empatica_signal(file_path, column_names):
    df = pd.read_csv(file_path, header=None)
    start_time = df.iloc[0, 0]
    frequency = df.iloc[1, 0]
    signal = df.iloc[2:].reset_index(drop=True)
    signal = signal.astype(float)
    timestamps = [start_time + i / frequency for i in range(len(signal))]
    signal.insert(0, 'timestamp', timestamps)
    signal.columns = ['timestamp'] + column_names
    return signal

# Load sensor data
bvp_df = load_empatica_signal("S3_E4/BVP.csv", ['BVP'])
acc_df = load_empatica_signal("S3_E4/ACC.csv", ['ACC_x', 'ACC_y', 'ACC_z'])
hr_df = load_empatica_signal("S3_E4/HR.csv", ['HR'])

# Merge based on nearest timestamps
df = pd.merge_asof(bvp_df, acc_df, on='timestamp')
df = pd.merge_asof(df, hr_df, on='timestamp')
df.dropna(inplace=True)

# Feature selection
print("Extracting and normalizing features...")
features = df[['BVP', 'ACC_x', 'ACC_y', 'ACC_z']]
hr = df['HR']

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(features)

# Create sliding window sequences
def create_sequences(X, y, window_size=128, stride=64):
    X_seq, y_seq = [], []
    for i in range(0, len(X) - window_size, stride):
        X_seq.append(X[i:i+window_size])
        y_seq.append(np.mean(y[i:i+window_size]))
    return np.array(X_seq), np.array(y_seq)

print("Creating sliding window sequences...")
X_seq, y_seq = create_sequences(X, hr.to_numpy())
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)

# Flatten for ML models
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

# Train machine learning models
print("Training machine learning models...")
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Support Vector Regressor": SVR(),
    "XGBoost": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
}

results = {}
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train_flat, y_train)
    preds = model.predict(X_test_flat)

    # Calculate RMSE manually
    rmse = np.sqrt(mean_squared_error(y_test, preds))

    results[name] = {
        "MAE": mean_absolute_error(y_test, preds),
        "RMSE": rmse,
        "R2": r2_score(y_test, preds),
        "Corr": pearsonr(y_test, preds)[0]
    }

# Deep learning model (CNN + LSTM)
print("Training CNN + LSTM model...")

# Adjust input shape for CNN + LSTM
nn_model = Sequential([
    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
    MaxPooling1D(2),
    Dropout(0.2),
    LSTM(64, return_sequences=False),
    Dense(32, activation='relu'),
    Dense(1)
])

nn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
nn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)
y_pred_nn = nn_model.predict(X_test).flatten()

# Calculate RMSE manually for CNN + LSTM
rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))

results["CNN + LSTM"] = {
    "MAE": mean_absolute_error(y_test, y_pred_nn),
    "RMSE": rmse_nn,
    "R2": r2_score(y_test, y_pred_nn),
    "Corr": pearsonr(y_test, y_pred_nn)[0]
}

# Show results
metrics_df = pd.DataFrame(results).T
print("\nModel performance comparison:")
print(metrics_df)

# Plot
metrics_df.plot(kind='bar', figsize=(12, 6))
plt.title("Comparison of Heart Rate Prediction Models")
plt.ylabel("Metric Score")
plt.xticks(rotation=30)
plt.grid(True)
plt.tight_layout()
plt.show()

# Statistical comparison
print("\nStatistical comparison between CNN + LSTM and other models:")
for name in results:
    if name != "CNN + LSTM":
        t_stat, p_val = ttest_rel(
            results[name]['MAE'] * np.ones_like(y_pred_nn),
            results["CNN + LSTM"]['MAE'] * np.ones_like(y_pred_nn)
        )
        print(f"{name} vs CNN + LSTM: p-value = {p_val:.4f}")

# Install required packages
!pip install xgboost

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from scipy.stats import ttest_rel, pearsonr
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint
from google.colab import files
import zipfile
import os

# File upload
uploaded = files.upload()  # Upload S3_E4.zip

# Extract ZIP
zip_path = "S3_E4.zip"
extract_path = "S3_E4"
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Extracted files:", os.listdir(extract_path))

# Function to load Empatica signal with timestamp
def load_empatica_signal(file_path, column_names):
    df = pd.read_csv(file_path, header=None)
    start_time = df.iloc[0, 0]
    frequency = df.iloc[1, 0]
    signal = df.iloc[2:].reset_index(drop=True)
    signal = signal.astype(float)
    timestamps = [start_time + i / frequency for i in range(len(signal))]
    signal.insert(0, 'timestamp', timestamps)
    signal.columns = ['timestamp'] + column_names
    return signal

# Load sensor data
bvp_df = load_empatica_signal("S3_E4/BVP.csv", ['BVP'])
acc_df = load_empatica_signal("S3_E4/ACC.csv", ['ACC_x', 'ACC_y', 'ACC_z'])
hr_df = load_empatica_signal("S3_E4/HR.csv", ['HR'])

# Merge based on nearest timestamps
df = pd.merge_asof(bvp_df, acc_df, on='timestamp')
df = pd.merge_asof(df, hr_df, on='timestamp')
df.dropna(inplace=True)

# Feature selection
print("Extracting and normalizing features...")
features = df[['BVP', 'ACC_x', 'ACC_y', 'ACC_z']]
hr = df['HR']

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(features)

# Create sliding window sequences
def create_sequences(X, y, window_size=128, stride=64):
    X_seq, y_seq = [], []
    for i in range(0, len(X) - window_size, stride):
        X_seq.append(X[i:i+window_size])
        y_seq.append(np.mean(y[i:i+window_size]))
    return np.array(X_seq), np.array(y_seq)

print("Creating sliding window sequences...")
X_seq, y_seq = create_sequences(X, hr.to_numpy())
X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)

# Flatten for ML models
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

# Train machine learning models
print("Training machine learning models...")
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Support Vector Regressor": SVR(),
    "XGBoost": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
}

results = {}
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train_flat, y_train)
    preds = model.predict(X_test_flat)

    # Calculate RMSE manually
    rmse = np.sqrt(mean_squared_error(y_test, preds))

    results[name] = {
        "MAE": mean_absolute_error(y_test, preds),
        "RMSE": rmse,
        "R2": r2_score(y_test, preds),
        "Corr": pearsonr(y_test, preds)[0]
    }

# Deep learning model (CNN + LSTM)
print("Training CNN + LSTM model...")

# Adjust input shape for CNN + LSTM
nn_model = Sequential([
    Conv1D(32, 3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
    MaxPooling1D(2),
    Dropout(0.2),
    LSTM(64, return_sequences=False),
    Dense(32, activation='relu'),
    Dense(1)
])

nn_model.compile(optimizer='adam', loss='mse', metrics=['mae'])
nn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=1)
y_pred_nn = nn_model.predict(X_test).flatten()

# Calculate RMSE manually for CNN + LSTM
rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred_nn))

results["CNN + LSTM"] = {
    "MAE": mean_absolute_error(y_test, y_pred_nn),
    "RMSE": rmse_nn,
    "R2": r2_score(y_test, y_pred_nn),
    "Corr": pearsonr(y_test, y_pred_nn)[0]
}

# Show results
metrics_df = pd.DataFrame(results).T
print("\nModel performance comparison:")
print(metrics_df)

# Plot
metrics_df.plot(kind='bar', figsize=(12, 6))
plt.title("Comparison of Heart Rate Prediction Models")
plt.ylabel("Metric Score")
plt.xticks(rotation=30)
plt.grid(True)
plt.tight_layout()
plt.show()

# Statistical comparison
print("\nStatistical comparison between CNN + LSTM and other models:")
for name in results:
    if name != "CNN + LSTM":
        t_stat, p_val = ttest_rel(
            results[name]['MAE'] * np.ones_like(y_pred_nn),
            results["CNN + LSTM"]['MAE'] * np.ones_like(y_pred_nn)
        )
        print(f"{name} vs CNN + LSTM: p-value = {p_val:.4f}")